{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file,'r') as f:\n",
    "        data=f.read()\n",
    "    \n",
    "    return data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences= load_data('small_vocab_en.txt')\n",
    "french_sentences = load_data('small_vocab_fr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the lime is her least liked fruit , but the banana is my least liked .\n",
      "la chaux est son moins aimÃ© des fruits , mais la banane est mon moins aimÃ©.\n"
     ]
    }
   ],
   "source": [
    "print(eng_sentences[10])\n",
    "print(french_sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English Words : 1823250\n",
      "Total French Words : 1961295\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "eng_words= collections.Counter([word for sentence in eng_sentences for word in sentence.split()])\n",
    "french_words= collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('Total English Words : {}'.format(len([word for sentence in eng_sentences for word in sentence.split()])))\n",
    "print('Total French Words : {}'.format(len([word for sentence in french_sentences for word in sentence.split()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional,RepeatVector,GRU,TimeDistributed\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \n",
    "    tokenizer =Tokenizer(num_words=None,char_level=False)\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    sequences =tokenizer.texts_to_sequences(x)\n",
    "    return sequences,tokenizer\n",
    "\n",
    "def pad(x,length=None):\n",
    "    pad=pad_sequences(x,maxlen=length,padding='post',truncating='post')\n",
    "    return pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x,y):\n",
    "    preprocess_x,x_tok=tokenize(x)\n",
    "    preprocess_y,y_tok=tokenize(y)\n",
    "    \n",
    "    preprocess_x= pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "    \n",
    "    \n",
    "    preprocess_y=preprocess_y.reshape(*preprocess_y.shape,1)\n",
    "    \n",
    "    return preprocess_x,preprocess_y,x_tok,y_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =preprocess(eng_sentences, french_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137861, 15)\n",
      "(137861, 21, 1)\n"
     ]
    }
   ],
   "source": [
    "print(preproc_english_sentences.shape)\n",
    "print(preproc_french_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelB(input_shape,output_shape,english_vocab_size,french_vocab_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size,output_shape,input_length=input_shape[1:][0]))\n",
    "    model.add(Bidirectional(GRU(english_vocab_size, return_sequences=False),input_shape=input_shape[1:]))\n",
    "    model.add(Dense(french_vocab_size,activation='relu'))\n",
    "    model.add(RepeatVector(output_shape))\n",
    "    model.add(Bidirectional(GRU(english_vocab_size,return_sequences= True)))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size,activation = 'softmax')))\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,optimizer = Adam(10e-3),metrics=['acc'])\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(x,y,x_tok,y_tok):\n",
    "    \n",
    "    model= modelB(x.shape,y.shape[1],len(x_tok.word_index)+1,len(y_tok.word_index)+1)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.fit(x,y,batch_size=1024,epochs=20,validation_split=0.2)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 15, 21)            4200      \n",
      "_________________________________________________________________\n",
      "bidirectional_26 (Bidirectio (None, 400)               267600    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 346)               138746    \n",
      "_________________________________________________________________\n",
      "repeat_vector_13 (RepeatVect (None, 21, 346)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 21, 400)           657600    \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 21, 346)           138746    \n",
      "=================================================================\n",
      "Total params: 1,206,892\n",
      "Trainable params: 1,206,892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/20\n",
      "110288/110288 [==============================] - 20s 182us/sample - loss: 2.6055 - acc: 0.4620 - val_loss: 1.5343 - val_acc: 0.5955\n",
      "Epoch 2/20\n",
      "110288/110288 [==============================] - 13s 122us/sample - loss: 1.2411 - acc: 0.6561 - val_loss: 1.0615 - val_acc: 0.6975\n",
      "Epoch 3/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.9552 - acc: 0.7225 - val_loss: 1.2555 - val_acc: 0.6479\n",
      "Epoch 4/20\n",
      "110288/110288 [==============================] - 13s 122us/sample - loss: 0.8227 - acc: 0.7548 - val_loss: 0.7008 - val_acc: 0.7846\n",
      "Epoch 5/20\n",
      "110288/110288 [==============================] - 14s 122us/sample - loss: 0.6392 - acc: 0.8023 - val_loss: 0.6076 - val_acc: 0.8119\n",
      "Epoch 6/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.5121 - acc: 0.8409 - val_loss: 0.4445 - val_acc: 0.8621\n",
      "Epoch 7/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.3896 - acc: 0.8784 - val_loss: 0.3605 - val_acc: 0.8878\n",
      "Epoch 8/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.3013 - acc: 0.9071 - val_loss: 0.2648 - val_acc: 0.9203\n",
      "Epoch 9/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.2369 - acc: 0.9270 - val_loss: 0.2273 - val_acc: 0.9296\n",
      "Epoch 10/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.2425 - acc: 0.9243 - val_loss: 0.2378 - val_acc: 0.9265\n",
      "Epoch 11/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.1801 - acc: 0.9443 - val_loss: 0.1708 - val_acc: 0.9477\n",
      "Epoch 12/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.1743 - acc: 0.9460 - val_loss: 0.1798 - val_acc: 0.9456\n",
      "Epoch 13/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.1613 - acc: 0.9502 - val_loss: 0.2086 - val_acc: 0.9361\n",
      "Epoch 14/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.1408 - acc: 0.9565 - val_loss: 0.1427 - val_acc: 0.9572\n",
      "Epoch 15/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.1366 - acc: 0.9575 - val_loss: 0.1733 - val_acc: 0.9495\n",
      "Epoch 16/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.1279 - acc: 0.9603 - val_loss: 0.1302 - val_acc: 0.9611\n",
      "Epoch 17/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.1065 - acc: 0.9671 - val_loss: 0.1642 - val_acc: 0.9510\n",
      "Epoch 18/20\n",
      "110288/110288 [==============================] - 14s 123us/sample - loss: 0.1412 - acc: 0.9565 - val_loss: 0.1318 - val_acc: 0.9606\n",
      "Epoch 19/20\n",
      "110288/110288 [==============================] - 18s 162us/sample - loss: 0.1128 - acc: 0.9657 - val_loss: 0.1263 - val_acc: 0.9627\n",
      "Epoch 20/20\n",
      "110288/110288 [==============================] - 18s 159us/sample - loss: 0.1136 - acc: 0.9653 - val_loss: 0.1222 - val_acc: 0.9638\n",
      "inde est pluvieux en juin et il est parfois chaud en novembre <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "model =predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)\n",
    "\n",
    "\n",
    "y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
    "y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "sentence = 'india is rainy during june  and it is sometimes warm in november '\n",
    "sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
    "sentence = pad_sequences([sentence], maxlen=preproc_english_sentences.shape[-1], padding='post')\n",
    "sentences = np.array([sentence[0]])\n",
    "predictions = model.predict(sentences, len(sentences))\n",
    "print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
